[
  {
    "path": "posts/2021-03-13-break-free-from-plastic/",
    "title": "Break Free From Plastic",
    "description": "In early January, I participated in #TidyTuesday. Here's the creative plot I made and how I made it.",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\nStep 1: Read in and wrangle data\nFirst, I read in and wrangled data from the TidyTuesday master github page. The data are from the organization Break Free From Plastic.\n\n\nShow code\n\n#read in data\nplastics <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-26/plastics.csv') %>% \n  #make data tidy\n  pivot_longer(empty:grand_total, \n               names_to = 'plastic_type', \n               values_to = 'count',\n               values_drop_na = TRUE) %>% \n  filter(count != 0)\n\nplastics_ps <- plastics %>% \n  select(-c(num_events, volunteers, year)) %>%   #remove num_events and volunteers columns\n  filter(!(parent_company %in% c(\"Grand Total\", \"null\", \"Null\", \"Unbranded\", \"NULL\", \"Assorted\"))) %>% #remove rows with unknown company names\n  filter(plastic_type == \"ps\") %>%   #only include polystyrene\n  filter(!(country == \"EMPTY\")) %>%   #drop rows with no country name\n  group_by(country) %>% \n  summarize(\n    total = sum(count)\n  )   #create a summary table of amount of polystyrene found in each country\n\n\n\nStep 2: Make a bubble plot\nIn the spirit of #TidyTuesday, I set out to make a fun graph I’d never made before, so I made a bubble plot using the packcircles package. I then followed the tutorial posted in R Graph Gallery\nThe image I used as my background is from The Huffington Post.\n\n\nShow code\n\n#see what sample data from instructions looks like\ndata <- data.frame(group=paste(\"Group\", letters[1:20]), value=sample(seq(1,100),20))  #wooo its the same format as my pastics data thank god\n\n# Generate the layout. This function return a dataframe with one line per bubble. \n# It gives its center (x and y) and its radius, proportional of the value\npacking <- circleProgressiveLayout(plastics_ps$total, sizetype = 'area')\n\n# We can add these packing information to the initial data frame\nplastic_circle <- cbind(plastics_ps, packing)\n\n# Check that radius is proportional to value. We don't want a linear relationship, since it is the AREA that must be proportionnal to the value\n#plot(plastic_circle$radius, plastic_circle$total)\n\n# The next step is to go from one center + a radius to the coordinates of a circle that\n# is drawn by a multitude of straight lines.\ndat.gg <- circleLayoutVertices(packing, npoints=50)\n\ndat.gg$total <- rep(plastics_ps$total, each = 51)\n\n#read in image\nlandfill <- jpeg::readJPEG(\"landfill.jpg\")\n\n\n# Make the plot\nggplot() + \n  #Make the bubles\n  background_image(landfill) +\n  geom_polygon(data = dat.gg, \n               aes(x, y, group = id, fill = as.factor(id)), alpha = 0.95) +\n  \n  #Add text in the center of each bubble + control its size\n  geom_text(data = plastic_circle, aes(x, y, size = total, label = country)) +\n  scale_size_continuous(range = c(1,4)) +\n  \n  #General theme\n  #scale_colour_viridis_b() +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  coord_equal() +\n  labs(title = \"Polystyrene (aka Stryrofoam) found in countries around the world\",\n       subtitle = \"Data collected by #BreakFreeFromPlastic\")\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-03-13-break-free-from-plastic/break-free-from-plastic_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-13T18:24:27-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-water-levels-at-mono-lake/",
    "title": "Water Levels at Mono Lake",
    "description": "In my very first R class in Fall 2020, I made a highly customized graph showing changes in water level at Mono Lake in CA since 1850. Here's how I made it!",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\nStep 1: Read in and wrangle data\n\n\nShow code\n\n# Read in data\nmono_lake <- read.csv((\"mono_lake_annual_levels.csv\"), skip = 5)\n\n# Clean up data formatting\nmono_lake_clean <- mono_lake %>% \n  clean_names() %>%                        #change column names to lowercase_snake_case\n  select(year, lake_level_feet_above_sea_level, stable_level, vertical_change) %>%   #keep relevant variables\n  mutate(land_bridge = 6377) %>% \n  mutate(shrimp_decrease = 6360) %>% \n  mutate(acute = 6350)\n\n# Create a data frame with annotation info for horizontal lines on the graph\nannotation_lines <- data.frame(\n  x = c(2000, 2000, 2000, 2000),\n  y = c(6392, 6377, 6360, 6350),\n  label = c(\"Stable Water Level\", \"Land Bridge\", \"Food Limit Starts\", \"Acute Food Limits\")\n)\n\n# Create a data frame with annotation info for points on the graph\nannotation_points <- data.frame(\n  x = c(1958, 1986),\n  y = c(6423, 6384),\n  label = c(\"LADWP diverts water\", \"Court ruling\")\n)\n\n\n\nStep 2: Create Visualization\n\n\nShow code\n\nmono_lake_clean %>% \n  ggplot(aes(x = year)) +\n  #geom_point(aes(y = lake_level_feet_above_sea_level), color = \"deepskyblue1\", size = 2.5, alpha = .5) +\n  geom_line(aes(y = lake_level_feet_above_sea_level),  #insert line for lake level\n            color = \"deepskyblue4\",\n            size = 1) +\n  annotate(geom = \"point\", x = 1941, y = 6417,  #add point for when LADWP started diverting water\n           size = 4,\n           color = \"darkblue\") +\n  annotate(geom = \"point\", x = 1983, y = 6378.60,         #add a point for when CA Sumpreme court ruled in favor of Mono Lake Committee\n           size = 4,\n           color = \"darkblue\") +\n  geom_ribbon(aes(ymin = -Inf,                    #fill in area below lake level line\n                ymax = lake_level_feet_above_sea_level),\n              fill = \"deepskyblue3\",\n              alpha = .3) +\n  geom_line(aes(y = stable_level),           #add a line for where the stable water level is\n            color = \"grey10\",\n            size = .7) +\n  geom_line(aes(y = land_bridge),            #add a line for where the water receeds below a land bridge\n            color = \"darkgoldenrod\",\n            size = 0.7) +\n  geom_line (aes(y = shrimp_decrease),        #add a line for where brine shrimp population starts decreasing\n             color = \"deeppink3\",\n             size = 0.7) +\n  geom_line(aes(y = acute),                   #add a line for where brine shrimp population decrease has acute effects\n            color = \"darkorchid3\",\n            size = 0.7) +\n  geom_label(data = annotation_lines, aes(x = x, y = y, label = label),   #add text for labels for two data points\n             size = 3.5, color = \"chocolate3\", fontface = \"bold\") +\n  geom_text(data = annotation_points, aes(x = x, y = y, label = label),      #add text for labels for horizontal lines representing key water levels\n             size = 3.5, color = \"black\", fontface = \"bold\", angle = 10) +\n  labs(x = \"Year\",              #add labels\n       y = \"Feet Above Sea Level\",\n       title = \"Change in Water Level at Mono Lake\",\n       caption = \"Figure 1: Mono Lake water levels meaured from 1850 to 2017.\",\n       subtitle = \"Mono County, CA (Kootzaduka’a Territory)\") +\n  theme_classic() +\n  theme(axis.text = element_text(size = 10),    #change font sizes\n        axis.title = element_text(size = 12),\n        plot.title = element_text(size = 14))\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-13T18:36:48-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-25-working-with-rasters/",
    "title": "Working with Rasters",
    "description": "Last post I wanted to share my coding skills with shapefiles, and today I want to share my skills working wtih rasters. In the following post, I wrangle cetacean probability raster data from AquaMaps to create a map of cetacean species richness off the coast of California.",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-02-25",
    "categories": [],
    "contents": "\nStep 1: Read in Data\nFirst I read in all the CA cetacean .tif files from AquaMaps and save them as a RasterStack using raster. The data shows the probability of occurrence of 35 cetacean species that can be found along the California coast. I also read in a shapefile of countries of the world using rnaturalearth.\n\n\nShow code\n\n#Pull all .tif files from folder\nmy_files <- list.files(path = \"ca_cetaceans\", full.names = TRUE)\n\n#Use `raster::stack` to rasterize all the layers at once\ncetacean_stack <- raster::stack(my_files)\n\n#Read in shapefile using `rnaturalearth`\ncoastline <- ne_countries(scale = 110, returnclass = \"sf\")\n\n#View data\n#cetacean_stack\n#summary(cetacean_stack)\n\n#Preliminary visualization of data\n#plot(cetacean_stack)\n\n\n\nStep 2: Filer the Data\nNext I filter the probability data using a manually created filter function. I filter the data so that if the probability of occurrence is greater than or equal to 0.75, the cetacean species is logged as being present in that raster square. I then add up all the filtered rasters to create one RasterLayer.\n\n\nShow code\n\n#Create a filter function \nis_present <- function(x, thresh = 0.75) {\n    y <- ifelse(x >= thresh, 1, NA)\n    return(y)\n}\n\n#filter the raster\ncetacean_present <- calc(cetacean_stack, fun = is_present)\n\n#add up all the raster layers\ncetacean_prob <- calc(cetacean_present, fun = sum, na.rm = TRUE)\n\n#change the initial raster into a dataframe\ncetacean_stack_df <- raster::rasterToPoints(cetacean_stack) %>%\n  as.data.frame()\n\n#change the filtered raster layer into a dataframe\ncetacean_prob_df <- raster::rasterToPoints(cetacean_prob) %>%\n  as.data.frame()\n\n#cetecean_prob_df[is.na(cetecean_prob_df[])] <- 0\n\n\n\nStep 3: Create a Static Map\nLastly I use ggplot2 and rnaturalearth to create a map of the cetacean species richness along the coast of California.\n\n\nShow code\n\nggplot() +\n  geom_raster(data = cetacean_prob_df, aes(x = x, y = y, fill = layer)) +\n  geom_sf(data = coastline) +\n  coord_sf(expand = 0, xlim = c(-130,-110), ylim = c(28,45)) +\n  scale_fill_gradient2(low = 'yellow', mid = \"red\", high =  'purple', midpoint = 13) +\n  labs(\n    x = \"Latitude\",\n    y = \"Longitude\",\n    fill = \"Species Richness\",\n    title = \"Cetacean species richness along the California coast\",\n    caption = \"Fig. 1: Cetacean species richness calculated using a 0.75 probability threshold.\"\n  ) +\n  theme_minimal()\n\n\n\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": "posts/2021-02-25-working-with-rasters/working-with-rasters_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-13T17:27:34-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-24-oil-spill-spatial-analysis/",
    "title": "Oil Spill Spatial Analysis",
    "description": "Another example of my coding skills I'm excited to share are my spatial maping skills using shapefiles. The following is also an assignment from my advanced data analysis course.",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-02-24",
    "categories": [],
    "contents": "\nStep 1: Read in data\nFirst, I use the sf and here packages to read in shapefiles of the State of California and Oil Spill Incident Tracking data. I then use st_transform to make sure both shapefiles have the same CRS.\n\n\nShow code\n\n#Read in oil spill layer\nds394 <- st_read(\"ds394\", layer = \"ds394\") %>% \n  clean_names()\n\n#Check the projection:\n#st_crs(ds394) #NAD83\n\n# Read in the CA county data (TIGER shapefile):\nca_counties <- st_read(\"ca_counties\", layer = \"CA_Counties_TIGER2016\") %>% \n  clean_names()\n\n#Check the projection:\n#st_crs(ca_counties) #WGS 84\n\n#Make ca_counties match the projection of ds394\nca_counties <- st_transform(ca_counties, st_crs(ds394))\n\n#confirm that the projection is updated\n#st_crs(ca_counties) #NAD83 confirmed\n\n#Make a quick exploratory ggplot\n#ggplot() +\n  #geom_sf(data = ca_counties) +\n  #geom_sf(data = ds394)\n\n\n\nStep 2: Make an Interactive Map\nI used tmap to create an exploratory interactive map of oil spills in California.\n\n\nShow code\n\n#Set tmap mode to interactive viewing\ntmap_mode(\"view\")\n\n#Create exploratory interactive map\ntm_shape(ds394) +\n  tm_dots(\"localecoun\") #dots are colored based on the county the spill occured in\n\n\n\n\nStep 3: Make a chloropleth map\nHere I use ggplot2 and sf to create a chloropleth map to visualize the number of inland oil spill events in every county in California.\n\n\nShow code\n\n#assume no duplicate oil spill events\n\n#make a subset of relevant ca county data\nca_subset <- ca_counties %>% \n  select(name) %>%     #select variable of interest\n  rename(county_name = name) #rename variable\n\n#make a subset of relevant oil spill data\noil_subset <- ds394 %>% \n  select(localecoun, inlandmari) %>% #select variables of interest\n  filter(inlandmari == \"Inland\") %>%  #filter dataset to only include inland observations\n  rename(county_name = localecoun)  #rename county column to match ca_subset\n\n#Join the oil spill and ca county datasets by variable `county_name`\nca_oil <- ca_subset %>% \n  st_join(oil_subset)\n\n#Find count of oil spills observed by county. \noil_counts <- ca_oil %>% \n  count(county_name.y)\n\n#Create chloropleth map using ggplot\nggplot(data = oil_counts) +\n  geom_sf(aes(fill = n), color = \"gray100\", size = 0.05) +\n  scale_fill_gradientn(colors = c(\"gray90\", \"gray0\")) +\n  theme_minimal() +\n  labs(fill = \"Number of Oil Spills\",\n       title = \"Inland Oil Spills in California Counties\",\n       caption = \"Los Angeles County has the most oil spills out of all the CA Counties.\")\n\n\n\n\nReferences\nCalifornia Department of Fish and Game, Office of Spill Prevention and Response. (2009). Oil Spill Incident Tracking [ds394]. California Department of Fish and Game. Available at: https://map.dfg.ca.gov/metadata/ds0394.html\nUS Census Bureau, Department of Commerce. (2016). CA Counties TIGER Line Shapefile.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-13T18:48:13-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-20-text-sentiment-analysis/",
    "title": "Text & Sentiment Analysis",
    "description": "In Winter 2021, I took an advanced data analysis course through the UCSB Bren School. In that course, I learned how to conduce text and sentiment analyses. I tried out my new skills by analyzing *Pride and Prejudice* by Jane Austen. Check out my analysis below!",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\nStep 1: Get Pride and Prejudice\nI used a .pdf of Pride and Prejudice by Jane Austen from Project Gutenberg’s database of free, publicly available eBooks.\n(For this task, I chose to use the zombies-free version of Pride and Prejudice. I assume including the undead would highlight negative sentiments not expressed in original the text.)\n\n\n\nStep 2: Wrangle the Text\nAfter pulling the .pdf, I used the R packages tidyverse and tidytext to convert the text into a tidy dataframe. I then remove stopwords from the data frame and count the number of times each non-stop word is used.\n\n\n\nStep 3: Visualize the Language\nNext, I used ggplot2 to visualize the top 10 words used in Pride and Prejudice and create a word cloud of the top 100. Since the word “elizabeth” was used much more than any other word (222 more times more than the next most common word, “darcy”), I omitted it from the word cloud.\n\n\n\n\n\n\nStep 4: Sentiment Analysis\nLastly, I use the NRC lexicon to examine the sentiment of the words used in Pride and Prejudice.\n\n\n\nReferences\nAusten, Jane. (1998). Pride and Prejudice. Project Gutenberg. Retrieved February 19, 2021, from https://www.gutenberg.org/ebooks/1342. (originally published 1813).\nCrowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-23T11:45:56-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-06-welcome/",
    "title": "welcome",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Mukta Kelkar",
        "url": {}
      }
    ],
    "date": "2021-02-06",
    "categories": [],
    "contents": "\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-06T20:49:36-08:00",
    "input_file": {}
  }
]
